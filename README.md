# Hi there 👋

## 🔭 I’m currently working with a data engineering stack that includes Hadoop (HDFS), Apache Spark, Airflow, and PostgreSQL to build scalable data pipelines and distributed processing workflows.

## 🛠️ Tech Stack I'm Working With

I'm currently focused on building data-intensive applications using a modern data engineering stack. My expertise and ongoing learning include:

## ⚙️ Core Technologies

- **Apache Hadoop (HDFS):**  
  Working with the Hadoop Distributed File System for scalable storage of large datasets across clusters.

- **Apache Spark:**  
  Leveraging Spark for distributed data processing, including understanding memory optimization, transformations, actions, and performance tuning.

- **Apache Airflow:**  
  Orchestrating complex ETL/ELT workflows using DAGs for scheduling and dependency management.

- **PostgreSQL:**  
  Using PostgreSQL for relational data storage and querying, including integration with pipelines and analytical tools.

## 🐍 Programming & Tools

- **Python:**  
  Developing data processing scripts, and workflow automation using Python, including functional programming.

- **Pandas:**  
  Data manipulation, cleaning, and transformation for intermediate processing steps or smaller-scale tasks.

- **Docker:**  
  Containerizing applications and services to ensure consistent deployment and easy environment management.

## 📁 Data Formats

- **Parquet, CSV, JSON:**  
  Working with various data formats for efficient storage, serialization, and exchange.

## 🔄 ETL/ELT Pipelines

- Designing and managing pipelines to ingest, transform, and load data from multiple sources into storage/analytics platforms.

## 🎯 Learning Goals

- **Apache Kafka:**  
  Exploring real-time data streaming and message brokering.
---

🚀 *Always exploring new ways to optimize data pipelines and build scalable, production-ready workflows.*

## 📫 How to reach me: niksu.831@gmail.com
